{
  "name": "pyfunction_test",
  "endpoints": [
    {
      "envSelection": {
        "envMode": "EXPLICIT_ENV",
        "envName": "py37"
      },
      "testQueries": [
        {
          "name": "Query #2",
          "q": {
            "project": "GIT",
            "dbName": "snowflake"
          }
        }
      ],
      "inputFolderRefs": [],
      "userFunctionName": "api_py_function",
      "code": "import dataiku\nimport dataikuapi\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\n\n# Connect to DSS and get a client\ndataiku.set_remote_dss(\"http://localhost:11600\", \"VRQcIPcI9645bnyPUjO1VXN70Ys70fVH\")\nclient \u003d dataiku.api_client()\n\n# Create a dataset by passing the projet Obj, the name of the db and the connection\ndef createTempDb(project, dbName, connection): \n        try:\n            # Build dataset\n            builder \u003d project.new_managed_dataset(\"record_temp\")\n            builder.with_store_into(\"filesystem_managed\") # Choose connection to store dataset\n            builder.create()\n        except Exception as e:\n            print(\"WARNING: Caught exception when creating the temporary db. ERROR: \" + str(e))\n            \n# Replace the input dataset with another dataset\n# setDatasetInput(\"compute_orders_prepared\", \"orders\", \"record_temp\")\ndef setDatasetInput(project, recipe, oldDs, newDs):\n    # Replace the input dataset with record_temp\n    recipe \u003d project.get_recipe(recipe)\n    settings \u003d recipe.get_settings()\n    settings.replace_input(oldDs, newDs)  # \"olddatabase name\", \"single row record dataset\"\n    settings.save()\n    \n    \ndef api_py_function(projectKey, data):\n    project \u003d client.get_project(projectKey) #\u0027DKU_TUTORIAL_BASICS_103_1\u0027\n    \n    createTempDb(project, \"record_temp\", \"filesystem_managed\")\n    \n    df \u003d pd.DataFrame(data\u003d[data])\n    dataiku.Dataset(project.project_key + \u0027.\u0027 + \"record_temp\").write_with_schema(df)\n    \n    # Get handle on the dataset with the schema you want to copy\n    oldDataset \u003d dataiku.Dataset(project.project_key + \u0027.\u0027 + \"data\")\n#     oldDataset_schema \u003d oldDataset.get_dataframe()\n    oldDataset_schema \u003d next(oldDataset.iter_dataframes(chunksize\u003d10000))\n\n    # Write the schema to new_dataset\n    dataiku.Dataset(project.project_key + \u0027.\u0027 + \"record_temp\").write_schema_from_dataframe(oldDataset_schema)\n    \n    recipeStart \u003d \u0027score_gr_ro_vdr_join_only_golden_test_set_fix_engine_hours_prepped_limit_prepped_fc_joined_prepared_1\u0027\n    setDatasetInput(project, recipeStart, \"api_record\", \"record_temp\")\n    \n    ############ ZONE ########################################################################\n    flow \u003d project.get_flow() #default\n    zone \u003d flow.get_zone(\"default\")\n    zone_graph \u003d zone.get_graph()\n    for item in zone_graph.get_items_in_traversal_order():\n        if item[\u0027type\u0027] \u003d\u003d \u0027COMPUTABLE_DATASET\u0027:\n            #Define dataset to be built\n            dataset \u003d project.get_dataset(item[\u0027ref\u0027])\n            \n            # Builds the dataset recursively. Remove job type to do non-recursive\n            dataset.build(job_type\u003d\"RECURSIVE_BUILD\")\n\n    # Replace the temp input dataset with the original dataset\n    setDatasetInput(project, recipeStart, \"record_temp\", \"data\")\n    \n    # Read prediction row out of the dataset at the end of the inference\n    inferenceDb \u003d dataiku.Dataset(project.project_key + \u0027.\u0027 + \"predict_compcode_group_41_42_45_golden_test_set_output_prepared\")\n    for df in inferenceDb.iter_dataframes(chunksize\u003d10000):\n        # df is a dataframe of at most 10K rows.\n        prediction \u003d df.iloc[0][\u0027comp_code\u0027]\n    \n    \n    return prediction\n",
      "id": "pyfunction_test",
      "type": "PY_FUNCTION"
    }
  ],
  "authMethod": "PUBLIC",
  "authRealm": {
    "queryKeys": []
  },
  "oauth2Config": {
    "keysFormat": "JWKS_URI",
    "scopeClaimFormat": "STRING",
    "scopeClaimKey": "scope"
  },
  "versionTag": {
    "versionNumber": 7,
    "lastModifiedBy": {
      "login": "admin"
    },
    "lastModifiedOn": 1682431327938
  },
  "creationTag": {
    "versionNumber": 0,
    "lastModifiedBy": {
      "login": "admin"
    },
    "lastModifiedOn": 1682430365929
  },
  "tags": [],
  "customFields": {},
  "checklists": {
    "checklists": []
  }
}